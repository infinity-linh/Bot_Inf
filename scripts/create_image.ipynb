{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_brightness(img, alpha, beta):\n",
    "    img_new = np.asarray(alpha*img + beta, dtype=int)   # cast pixel values to int\n",
    "    img_new[img_new>255] = 255\n",
    "    img_new[img_new<0] = 0\n",
    "    return np.array(img_new, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_eye_left_1 = cv2.imread(\"D:/User/ESP32_DOIT/scripts/kitchenrobot.jpg\")\n",
    "image_eye_left_2 = cv2.imread(\"D:/User/ESP32_DOIT/scripts/smilerobot.jpg\")\n",
    "image_eye_left_1 = cv2.resize(image_eye_left_1, [523,630])\n",
    "image_eye_left_2 = cv2.resize(image_eye_left_2, [523,630])\n",
    "\n",
    "print(image_eye_left_1.shape)\n",
    "print(image_eye_left_2.shape)\n",
    "# image_processed = change_brightness(image, 1, 35)\n",
    "eyes = cv2.hconcat([image_eye_left_1, image_eye_left_2])\n",
    "\n",
    "cv2.imshow(\"image\", eyes)\n",
    "cv2.imwrite(\"imagerobot.png\", eyes)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = 'D:/User/ESP32_DOIT/scripts/image_102.txt'\n",
    "# read text file into pandas DataFrame\n",
    "df = pd.read_csv(path, sep=\" \", header=None)\n",
    "\n",
    "# display DataFrame\n",
    "print(df[0])\n",
    "for i in range(len(df[0])):\n",
    "    df[0][i] = 45\n",
    "    \n",
    "\n",
    "print(df[0])\n",
    "df.to_csv(path, sep=' ', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [ 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "         'hair drier', 'toothbrush' ]\n",
    "names[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "box1 = [100,100,100,100]\n",
    "box2 = [100,100,100,100]\n",
    "state = [0,0,0,0]\n",
    "df = pd.read_csv(\"D:/User/Bot_C/Res/scripts/control.csv\")\n",
    "if df.empty:\n",
    "    print('DataFrame is empty!')\n",
    "data_frame = []\n",
    "# # data_frame.append(box1)\n",
    "data_frame.append(box2)\n",
    "data_frame.append(state)\n",
    "data_frame = np.reshape(data_frame, (1,8))\n",
    "\n",
    "# # df2 = pd.DataFrame(data_frame, columns=['x1', 'x2', 'x3', 'x4',\n",
    "# #                                         'y1', 'y2', 'y3', 'y4',\n",
    "# #                                         'q1', 'q2', 'q3', 'q4'])\n",
    "df2 = pd.DataFrame(data_frame, columns=['c11', 'c12', 'c21', 'c22', 'q1', 'q2', 'q3', 'q4'])\n",
    "\n",
    "# df2 = pd.concat([df,df2])\n",
    "df2.to_csv(\"D:/User/Bot_C/Res/scripts/control.csv\", index=False)\n",
    "\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.liner1 = nn.Linear(8,128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.liner2 = nn.Linear(128,8)\n",
    "        self.liner3 = nn.Linear(8,4)\n",
    "\n",
    "        # self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.liner1(x))\n",
    "        out = self.relu(self.liner2(out))\n",
    "        out = self.relu(self.liner3(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN()\n",
    "tensor = torch.rand([1, 8])\n",
    "model(torch.tensor([X[0]], dtype=torch.float32))\n",
    "# \n",
    "\n",
    "# tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_ANN import ANN\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch\n",
    "def study(model, X, y, optimizer, losses, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # counter = 0\n",
    "    # print (\"@@@ Start train model @@@\")\n",
    "    for i, data in enumerate(X):\n",
    "        # counter += 1\n",
    "        image, label = torch.tensor(data,dtype=torch.float32), torch.tensor(y[i],dtype=torch.float32)\n",
    "        image = image.to(device)\n",
    "        label =label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        label_pre = model(image)\n",
    "        # print(label_pre.shape)\n",
    "        loss = losses(label_pre, label)\n",
    "        train_loss += loss.item()\n",
    "        # print(train_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(preds)\n",
    "    epoch_loss = train_loss / len(X)\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_data(Train_X, Train_y, batch):\n",
    "    X, y = [], []\n",
    "    XX, yy = [], []\n",
    "    j=0\n",
    "    while j < len(Train_y):\n",
    "        X.append(Train_X[j])\n",
    "        y.append(Train_y[j])\n",
    "        if (j+1)%batch==0:\n",
    "            XX.append(X)\n",
    "            yy.append(y)\n",
    "            X, y = [], []\n",
    "        j+=1\n",
    "    return np.array(XX), np.array(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 32, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"D:/User/DLBot/scripts/data/angles.csv\")\n",
    "# df.head()\n",
    "data = np.array(df)\n",
    "X = np.array(data[:,0:8]/400, dtype=np.double) \n",
    "y = np.array(data[:,8:12]/180, dtype=np.double)\n",
    "X_train, y_train = create_batch_data(X, y, 32)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "epoch:  0 loss:  0.0002618244167630716\n",
      "epoch:  10 loss:  0.0002618002545204945\n",
      "epoch:  20 loss:  0.0002617760910652578\n",
      "epoch:  30 loss:  0.0002617518875922542\n",
      "epoch:  40 loss:  0.0002617277641547844\n",
      "epoch:  50 loss:  0.0002617035497678444\n",
      "epoch:  60 loss:  0.00026167941420377855\n",
      "epoch:  70 loss:  0.00026165522285737097\n",
      "epoch:  80 loss:  0.0002616310860806455\n",
      "epoch:  90 loss:  0.0002616069262633876\n",
      "epoch:  100 loss:  0.0002615827943373006\n",
      "epoch:  110 loss:  0.00026155860905419104\n",
      "epoch:  120 loss:  0.00026153459354342584\n",
      "epoch:  130 loss:  0.00026151051254904206\n",
      "epoch:  140 loss:  0.0002614864642964676\n",
      "epoch:  150 loss:  0.0002614623711754878\n",
      "epoch:  160 loss:  0.00026143836172802065\n",
      "epoch:  170 loss:  0.00026141431953874417\n",
      "epoch:  180 loss:  0.0002613903755748955\n",
      "epoch:  190 loss:  0.0002613663176210442\n",
      "epoch:  200 loss:  0.00026134228755836375\n",
      "epoch:  210 loss:  0.0002613182526450449\n",
      "epoch:  220 loss:  0.0002612942565368333\n",
      "epoch:  230 loss:  0.00026127026285394095\n",
      "epoch:  240 loss:  0.00026124625098115456\n",
      "epoch:  250 loss:  0.0002612222694248582\n",
      "epoch:  260 loss:  0.0002611982587647314\n",
      "epoch:  270 loss:  0.0002611743038869463\n",
      "epoch:  280 loss:  0.0002611503441585228\n",
      "epoch:  290 loss:  0.00026112642566052574\n",
      "epoch:  300 loss:  0.00026110244531688903\n",
      "epoch:  310 loss:  0.00026107855713538203\n",
      "epoch:  320 loss:  0.00026105465803993866\n",
      "epoch:  330 loss:  0.00026103079289896414\n",
      "epoch:  340 loss:  0.00026100693382128765\n",
      "epoch:  350 loss:  0.00026098308808286674\n",
      "epoch:  360 loss:  0.0002609592241545518\n",
      "epoch:  370 loss:  0.0002609353529502793\n",
      "epoch:  380 loss:  0.00026091148174600676\n",
      "epoch:  390 loss:  0.00026088760811641504\n",
      "epoch:  400 loss:  0.0002608637866311862\n",
      "epoch:  410 loss:  0.00026083991057627526\n",
      "epoch:  420 loss:  0.00026081611698221724\n",
      "epoch:  430 loss:  0.0002607922760944348\n",
      "epoch:  440 loss:  0.0002607684315686735\n",
      "epoch:  450 loss:  0.000260744578554295\n",
      "epoch:  460 loss:  0.00026072070613736287\n",
      "epoch:  470 loss:  0.0002606968579736228\n",
      "epoch:  480 loss:  0.00026067294796424295\n",
      "epoch:  490 loss:  0.0002606490440181612\n",
      "epoch:  500 loss:  0.0002606252079810171\n",
      "epoch:  510 loss:  0.00026060136466791545\n",
      "epoch:  520 loss:  0.0002605776110916243\n",
      "epoch:  530 loss:  0.00026055381022160873\n",
      "epoch:  540 loss:  0.0002605300505820196\n",
      "epoch:  550 loss:  0.0002605062630512596\n",
      "epoch:  560 loss:  0.00026048245005464804\n",
      "epoch:  570 loss:  0.00026045870981761254\n",
      "epoch:  580 loss:  0.00026043501929962076\n",
      "epoch:  590 loss:  0.0002604112839132237\n",
      "epoch:  600 loss:  0.00026038757399267826\n",
      "epoch:  610 loss:  0.0002603638774113885\n",
      "epoch:  620 loss:  0.0002603402353997808\n",
      "epoch:  630 loss:  0.0002603165460944486\n",
      "epoch:  640 loss:  0.00026029290772081975\n",
      "epoch:  650 loss:  0.00026026937242325704\n",
      "epoch:  660 loss:  0.0002602458601662268\n",
      "epoch:  670 loss:  0.00026022232365600456\n",
      "epoch:  680 loss:  0.0002601988041230167\n",
      "epoch:  690 loss:  0.0002601752918659865\n",
      "epoch:  700 loss:  0.000260151758993743\n",
      "epoch:  710 loss:  0.000260128291605118\n",
      "epoch:  720 loss:  0.0002601047732847898\n",
      "epoch:  730 loss:  0.00026008129740754765\n",
      "epoch:  740 loss:  0.0002600577633226446\n",
      "epoch:  750 loss:  0.0002600342365136991\n",
      "epoch:  760 loss:  0.00026001072304400924\n",
      "epoch:  770 loss:  0.00025998720472368103\n",
      "epoch:  780 loss:  0.00025996369489196997\n",
      "epoch:  790 loss:  0.0002599402081007914\n",
      "epoch:  800 loss:  0.000259916735861528\n",
      "epoch:  810 loss:  0.0002598933242552448\n",
      "epoch:  820 loss:  0.00025986985565396026\n",
      "epoch:  830 loss:  0.00025984636279948364\n",
      "epoch:  840 loss:  0.0002598229596818176\n",
      "epoch:  850 loss:  0.00025979955413883243\n",
      "epoch:  860 loss:  0.00025977617042372003\n",
      "epoch:  870 loss:  0.0002597527757946712\n",
      "epoch:  880 loss:  0.0002597294260340277\n",
      "epoch:  890 loss:  0.00025970610416455503\n",
      "epoch:  900 loss:  0.00025968275804189034\n",
      "epoch:  910 loss:  0.0002596593961546508\n",
      "epoch:  920 loss:  0.00025963603548007086\n",
      "epoch:  930 loss:  0.0002596126602535757\n",
      "epoch:  940 loss:  0.00025958936506261426\n",
      "epoch:  950 loss:  0.00025956601772729\n",
      "epoch:  960 loss:  0.00025954272981228616\n",
      "epoch:  970 loss:  0.0002595193824769619\n",
      "epoch:  980 loss:  0.00025949609092397924\n",
      "epoch:  990 loss:  0.00025947275328993175\n",
      "epoch:  1000 loss:  0.0002594494690129068\n",
      "epoch:  1010 loss:  0.00025942617139662616\n",
      "epoch:  1020 loss:  0.00025940291986141045\n",
      "epoch:  1030 loss:  0.00025937960648055497\n",
      "epoch:  1040 loss:  0.0002593563452440624\n",
      "epoch:  1050 loss:  0.00025933306945565465\n",
      "epoch:  1060 loss:  0.0002593097912419277\n",
      "epoch:  1070 loss:  0.0002592865991270325\n",
      "epoch:  1080 loss:  0.00025926338518426445\n",
      "epoch:  1090 loss:  0.00025924019306936924\n",
      "epoch:  1100 loss:  0.00025921698518989916\n",
      "epoch:  1110 loss:  0.0002591938585586225\n",
      "epoch:  1120 loss:  0.0002591706809956425\n",
      "epoch:  1130 loss:  0.0002591475422377698\n",
      "epoch:  1140 loss:  0.00025912439499127987\n",
      "epoch:  1150 loss:  0.0002591012562334072\n",
      "epoch:  1160 loss:  0.0002590780750324484\n",
      "epoch:  1170 loss:  0.00025905496537840617\n",
      "epoch:  1180 loss:  0.00025903178539010696\n",
      "epoch:  1190 loss:  0.0002590086733107455\n",
      "epoch:  1200 loss:  0.0002589855757832993\n",
      "epoch:  1210 loss:  0.0002589624806811723\n",
      "epoch:  1220 loss:  0.00025893933828532073\n",
      "epoch:  1230 loss:  0.00025891625530978973\n",
      "epoch:  1240 loss:  0.00025889316627096076\n",
      "epoch:  1250 loss:  0.00025887008450808935\n",
      "epoch:  1260 loss:  0.00025884701002117555\n",
      "epoch:  1270 loss:  0.0002588239864659651\n",
      "epoch:  1280 loss:  0.00025880097261203144\n",
      "epoch:  1290 loss:  0.00025877789812511764\n",
      "epoch:  1300 loss:  0.0002587549073117164\n",
      "epoch:  1310 loss:  0.00025873183282480267\n",
      "epoch:  1320 loss:  0.00025870875955054845\n",
      "epoch:  1330 loss:  0.0002586856377699102\n",
      "epoch:  1340 loss:  0.00025866257662225206\n",
      "epoch:  1350 loss:  0.0002586395433657647\n",
      "epoch:  1360 loss:  0.00025861648221810657\n",
      "epoch:  1370 loss:  0.00025859348049076897\n",
      "epoch:  1380 loss:  0.0002585704666368353\n",
      "epoch:  1390 loss:  0.00025854745642088045\n",
      "epoch:  1400 loss:  0.00025852449228599045\n",
      "epoch:  1410 loss:  0.00025850150147258927\n",
      "epoch:  1420 loss:  0.0002584784851933364\n",
      "epoch:  1430 loss:  0.0002584555707774901\n",
      "epoch:  1440 loss:  0.0002584325969413233\n",
      "epoch:  1450 loss:  0.00025840967161154066\n",
      "epoch:  1460 loss:  0.00025838674385643873\n",
      "epoch:  1470 loss:  0.00025836386703304015\n",
      "epoch:  1480 loss:  0.00025834090532346937\n",
      "epoch:  1490 loss:  0.0002583180163734748\n",
      "epoch:  1500 loss:  0.0002582951444007146\n",
      "epoch:  1510 loss:  0.00025827224332412396\n",
      "epoch:  1520 loss:  0.0002582493361842353\n",
      "epoch:  1530 loss:  0.0002582264569355175\n",
      "epoch:  1540 loss:  0.00025820361042860895\n",
      "epoch:  1550 loss:  0.00025818068509882625\n",
      "epoch:  1560 loss:  0.0002581579271160687\n",
      "epoch:  1570 loss:  0.00025813513032820384\n",
      "epoch:  1580 loss:  0.00025811234809225425\n",
      "epoch:  1590 loss:  0.00025808956221832585\n",
      "epoch:  1600 loss:  0.00025806679817227024\n",
      "epoch:  1610 loss:  0.0002580440365515339\n",
      "epoch:  1620 loss:  0.0002580213464777141\n",
      "epoch:  1630 loss:  0.0002579986551912346\n",
      "epoch:  1640 loss:  0.00025797594086422276\n",
      "epoch:  1650 loss:  0.00025795326898029697\n",
      "epoch:  1660 loss:  0.00025793063347615924\n",
      "epoch:  1670 loss:  0.0002579078912579765\n",
      "epoch:  1680 loss:  0.00025788519754617784\n",
      "epoch:  1690 loss:  0.00025786246987991035\n",
      "epoch:  1700 loss:  0.00025783977495545213\n",
      "epoch:  1710 loss:  0.00025781707881833427\n",
      "epoch:  1720 loss:  0.00025779439238249324\n",
      "epoch:  1730 loss:  0.00025777168533143896\n",
      "epoch:  1740 loss:  0.0002577489710044271\n",
      "epoch:  1750 loss:  0.0002577262857812457\n",
      "epoch:  1760 loss:  0.0002577036599783848\n",
      "epoch:  1770 loss:  0.00025768106812999275\n",
      "epoch:  1780 loss:  0.0002576584277752166\n",
      "epoch:  1790 loss:  0.0002576357946963981\n",
      "epoch:  1800 loss:  0.0002576131179618339\n",
      "epoch:  1810 loss:  0.0002575904897336538\n",
      "epoch:  1820 loss:  0.0002575678214877068\n",
      "epoch:  1830 loss:  0.0002575451568797386\n",
      "epoch:  1840 loss:  0.00025752249348443\n",
      "epoch:  1850 loss:  0.00025749983251444064\n",
      "epoch:  1860 loss:  0.0002574772358154102\n",
      "epoch:  1870 loss:  0.00025745462941510294\n",
      "epoch:  1880 loss:  0.00025743202058947645\n",
      "epoch:  1890 loss:  0.000257409423890446\n",
      "epoch:  1900 loss:  0.0002573867847483295\n",
      "epoch:  1910 loss:  0.0002573641650087666\n",
      "epoch:  1920 loss:  0.0002573415113147348\n",
      "epoch:  1930 loss:  0.00025731886489666067\n",
      "epoch:  1940 loss:  0.0002572962390937998\n",
      "epoch:  1950 loss:  0.00025727359995168325\n",
      "epoch:  1960 loss:  0.00025725097536148195\n",
      "epoch:  1970 loss:  0.0002572283920017071\n",
      "epoch:  1980 loss:  0.00025720584623438\n",
      "epoch:  1990 loss:  0.00025718331380630843\n",
      "epoch:  2000 loss:  0.00025716074257312965\n",
      "epoch:  2010 loss:  0.0002571382101450581\n",
      "epoch:  2020 loss:  0.0002571156801423058\n",
      "epoch:  2030 loss:  0.00025709318894466077\n",
      "epoch:  2040 loss:  0.00025707066743052565\n",
      "epoch:  2050 loss:  0.0002570481859341574\n",
      "epoch:  2060 loss:  0.00025702574324289645\n",
      "epoch:  2070 loss:  0.00025700326417184743\n",
      "epoch:  2080 loss:  0.00025698079358941567\n",
      "epoch:  2090 loss:  0.0002569583169436858\n",
      "epoch:  2100 loss:  0.0002569358912296593\n",
      "epoch:  2110 loss:  0.00025691344611307915\n",
      "epoch:  2120 loss:  0.0002568910495028831\n",
      "epoch:  2130 loss:  0.00025686861530023936\n",
      "epoch:  2140 loss:  0.0002568462138394049\n",
      "epoch:  2150 loss:  0.0002568238463330393\n",
      "epoch:  2160 loss:  0.00025680143759624724\n",
      "epoch:  2170 loss:  0.00025677898156573065\n",
      "epoch:  2180 loss:  0.00025675652917319286\n",
      "epoch:  2190 loss:  0.0002567341847073597\n",
      "epoch:  2200 loss:  0.00025671177354524843\n",
      "epoch:  2210 loss:  0.00025668941331484046\n",
      "epoch:  2220 loss:  0.00025666703731985763\n",
      "epoch:  2230 loss:  0.00025664466981349204\n",
      "epoch:  2240 loss:  0.00025662231807170127\n",
      "epoch:  2250 loss:  0.0002565999081222496\n",
      "epoch:  2260 loss:  0.00025657756729439524\n",
      "epoch:  2270 loss:  0.00025655527861090377\n",
      "epoch:  2280 loss:  0.00025653294505900703\n",
      "epoch:  2290 loss:  0.00025651061757040833\n",
      "epoch:  2300 loss:  0.00025648831676032086\n",
      "epoch:  2310 loss:  0.0002564660329274678\n",
      "epoch:  2320 loss:  0.0002564437830490836\n",
      "epoch:  2330 loss:  0.0002564214907276134\n",
      "epoch:  2340 loss:  0.00025639927237837884\n",
      "epoch:  2350 loss:  0.00025637706615574035\n",
      "epoch:  2360 loss:  0.0002563548575077827\n",
      "epoch:  2370 loss:  0.0002563326682623786\n",
      "epoch:  2380 loss:  0.0002563104547637825\n",
      "epoch:  2390 loss:  0.00025628831766274135\n",
      "epoch:  2400 loss:  0.00025626613811861415\n",
      "epoch:  2410 loss:  0.00025624398282767896\n",
      "epoch:  2420 loss:  0.0002562218348127014\n",
      "epoch:  2430 loss:  0.0002561996989243198\n",
      "epoch:  2440 loss:  0.0002561775569726403\n",
      "epoch:  2450 loss:  0.00025615548171723884\n",
      "epoch:  2460 loss:  0.00025613334886050626\n",
      "epoch:  2470 loss:  0.00025611123055568896\n",
      "epoch:  2480 loss:  0.000256089184404118\n",
      "epoch:  2490 loss:  0.0002560670927778119\n",
      "epoch:  2500 loss:  0.0002560450199477297\n",
      "epoch:  2510 loss:  0.0002560228955796144\n",
      "epoch:  2520 loss:  0.000256000838514107\n",
      "epoch:  2530 loss:  0.00025597873294221546\n",
      "epoch:  2540 loss:  0.00025595666556910146\n",
      "epoch:  2550 loss:  0.00025593460122763645\n",
      "epoch:  2560 loss:  0.0002559125623520231\n",
      "epoch:  2570 loss:  0.00025589058350306004\n",
      "epoch:  2580 loss:  0.0002558685622110109\n",
      "epoch:  2590 loss:  0.00025584657851140946\n",
      "epoch:  2600 loss:  0.0002558245851105312\n",
      "epoch:  2610 loss:  0.00025580260868688737\n",
      "epoch:  2620 loss:  0.000255780682588617\n",
      "epoch:  2630 loss:  0.0002557587019206646\n",
      "epoch:  2640 loss:  0.0002557367770350538\n",
      "epoch:  2650 loss:  0.0002557148497241239\n",
      "epoch:  2660 loss:  0.0002556929533360138\n",
      "epoch:  2670 loss:  0.00025567103148205206\n",
      "epoch:  2680 loss:  0.00025564916541043203\n",
      "epoch:  2690 loss:  0.00025562727508561994\n",
      "epoch:  2700 loss:  0.00025560535383798805\n",
      "epoch:  2710 loss:  0.00025558351080690045\n",
      "epoch:  2720 loss:  0.00025556166595682345\n",
      "epoch:  2730 loss:  0.0002555397998852034\n",
      "epoch:  2740 loss:  0.00025551796109842445\n",
      "epoch:  2750 loss:  0.0002554961580851038\n",
      "epoch:  2760 loss:  0.00025547435689077247\n",
      "epoch:  2770 loss:  0.00025545259268255904\n",
      "epoch:  2780 loss:  0.00025543081149711117\n",
      "epoch:  2790 loss:  0.0002554090218230461\n",
      "epoch:  2800 loss:  0.0002553872357869598\n",
      "epoch:  2810 loss:  0.0002553654473255544\n",
      "epoch:  2820 loss:  0.0002553436679590959\n",
      "epoch:  2830 loss:  0.0002553219061762017\n",
      "epoch:  2840 loss:  0.00025530007769702934\n",
      "epoch:  2850 loss:  0.00025527831773312454\n",
      "epoch:  2860 loss:  0.00025525663962374284\n",
      "epoch:  2870 loss:  0.00025523487480919965\n",
      "epoch:  2880 loss:  0.00025521318093524314\n",
      "epoch:  2890 loss:  0.0002551913694333052\n",
      "epoch:  2900 loss:  0.0002551696670707315\n",
      "epoch:  2910 loss:  0.00025514795803853\n",
      "epoch:  2920 loss:  0.00025512622839111526\n",
      "epoch:  2930 loss:  0.00025510453451715875\n",
      "epoch:  2940 loss:  0.00025508282366596785\n",
      "epoch:  2950 loss:  0.00025506118618068285\n",
      "epoch:  2960 loss:  0.0002550395583966747\n",
      "epoch:  2970 loss:  0.00025501789544553805\n",
      "epoch:  2980 loss:  0.00025499625189695507\n",
      "epoch:  2990 loss:  0.0002549744973900185\n",
      "epoch:  3000 loss:  0.0002549528289819136\n",
      "epoch:  3010 loss:  0.00025493117451939423\n",
      "epoch:  3020 loss:  0.0002549095570429927\n",
      "epoch:  3030 loss:  0.00025488795048052754\n",
      "epoch:  3040 loss:  0.00025486635604465846\n",
      "epoch:  3050 loss:  0.00025484470946442644\n",
      "epoch:  3060 loss:  0.00025482309380701434\n",
      "epoch:  3070 loss:  0.0002548014805749214\n",
      "epoch:  3080 loss:  0.00025477991342389333\n",
      "epoch:  3090 loss:  0.00025475836325009976\n",
      "epoch:  3100 loss:  0.0002547368106509869\n",
      "epoch:  3110 loss:  0.00025471521803410724\n",
      "epoch:  3120 loss:  0.00025469370909074013\n",
      "epoch:  3130 loss:  0.0002546721504283293\n",
      "epoch:  3140 loss:  0.0002546505741823542\n",
      "epoch:  3150 loss:  0.00025462906281366787\n",
      "epoch:  3160 loss:  0.00025460753264875774\n",
      "epoch:  3170 loss:  0.000254586010366135\n",
      "epoch:  3180 loss:  0.0002545644789885652\n",
      "epoch:  3190 loss:  0.00025454295670594246\n",
      "epoch:  3200 loss:  0.0002545214380612985\n",
      "epoch:  3210 loss:  0.00025449991335335653\n",
      "epoch:  3220 loss:  0.0002544784232062132\n",
      "epoch:  3230 loss:  0.0002544568645438024\n",
      "epoch:  3240 loss:  0.0002544354047131492\n",
      "epoch:  3250 loss:  0.0002544139224482933\n",
      "epoch:  3260 loss:  0.00025439240683529835\n",
      "epoch:  3270 loss:  0.00025437095852491137\n",
      "epoch:  3280 loss:  0.0002543494653461191\n",
      "epoch:  3290 loss:  0.0002543280358319559\n",
      "epoch:  3300 loss:  0.00025430652567592915\n",
      "epoch:  3310 loss:  0.0002542850506870309\n",
      "epoch:  3320 loss:  0.0002542635738791432\n",
      "epoch:  3330 loss:  0.00025424209161428735\n",
      "epoch:  3340 loss:  0.0002542206360279427\n",
      "epoch:  3350 loss:  0.0002541991749846299\n",
      "epoch:  3360 loss:  0.000254177778212276\n",
      "epoch:  3370 loss:  0.0002541562947347605\n",
      "epoch:  3380 loss:  0.00025413487552820396\n",
      "epoch:  3390 loss:  0.00025411342054818914\n",
      "epoch:  3400 loss:  0.00025409188917061937\n",
      "epoch:  3410 loss:  0.0002540703802272522\n",
      "epoch:  3420 loss:  0.0002540488627952679\n",
      "epoch:  3430 loss:  0.00025402731747211266\n",
      "epoch:  3440 loss:  0.0002540057939768303\n",
      "epoch:  3450 loss:  0.00025398431656261283\n",
      "epoch:  3460 loss:  0.0002539628106508947\n",
      "epoch:  3470 loss:  0.00025394132899236865\n",
      "epoch:  3480 loss:  0.0002539198721933644\n",
      "epoch:  3490 loss:  0.00025389839356648736\n",
      "epoch:  3500 loss:  0.0002538769331295043\n",
      "epoch:  3510 loss:  0.00025385547026720207\n",
      "epoch:  3520 loss:  0.000253833997703623\n",
      "epoch:  3530 loss:  0.00025381261002621613\n",
      "epoch:  3540 loss:  0.00025379117566141457\n",
      "epoch:  3550 loss:  0.0002537697916219865\n",
      "epoch:  3560 loss:  0.000253748426985112\n",
      "epoch:  3570 loss:  0.00025372705325329054\n",
      "epoch:  3580 loss:  0.00025370570498732076\n",
      "epoch:  3590 loss:  0.0002536843470200741\n",
      "epoch:  3600 loss:  0.00025366302603894536\n",
      "epoch:  3610 loss:  0.0002536416808046245\n",
      "epoch:  3620 loss:  0.0002536203949906242\n",
      "epoch:  3630 loss:  0.00025359905763859086\n",
      "epoch:  3640 loss:  0.00025357777485623956\n",
      "epoch:  3650 loss:  0.0002535565229967081\n",
      "epoch:  3660 loss:  0.0002535352577979211\n",
      "epoch:  3670 loss:  0.00025351400108775124\n",
      "epoch:  3680 loss:  0.00025349283653971116\n",
      "epoch:  3690 loss:  0.0002534715525447003\n",
      "epoch:  3700 loss:  0.00025345035222320195\n",
      "epoch:  3710 loss:  0.00025342921374734334\n",
      "epoch:  3720 loss:  0.00025340802979674965\n",
      "epoch:  3730 loss:  0.00025338684220817714\n",
      "epoch:  3740 loss:  0.0002533656800854563\n",
      "epoch:  3750 loss:  0.0002533445494918851\n",
      "epoch:  3760 loss:  0.0002533234655857086\n",
      "epoch:  3770 loss:  0.0002533023798605427\n",
      "epoch:  3780 loss:  0.0002532813008050046\n",
      "epoch:  3790 loss:  0.0002532601835506891\n",
      "epoch:  3800 loss:  0.000253239040224192\n",
      "epoch:  3810 loss:  0.00025321800846237846\n",
      "epoch:  3820 loss:  0.00025319688150678604\n",
      "epoch:  3830 loss:  0.00025317580305757775\n",
      "epoch:  3840 loss:  0.00025315474886156153\n",
      "epoch:  3850 loss:  0.00025313367465666187\n",
      "epoch:  3860 loss:  0.0002531126616910721\n",
      "epoch:  3870 loss:  0.00025309162083431147\n",
      "epoch:  3880 loss:  0.0002530705866471787\n",
      "epoch:  3890 loss:  0.00025304955306637567\n",
      "epoch:  3900 loss:  0.00025302851827291306\n",
      "epoch:  3910 loss:  0.0002530075047009935\n",
      "epoch:  3920 loss:  0.000252986530540511\n",
      "epoch:  3930 loss:  0.0002529655345521557\n",
      "epoch:  3940 loss:  0.0002529445543283752\n",
      "epoch:  3950 loss:  0.00025292361169704236\n",
      "epoch:  3960 loss:  0.0002529026466315069\n",
      "epoch:  3970 loss:  0.00025288170218118466\n",
      "epoch:  3980 loss:  0.0002528607874410227\n",
      "epoch:  3990 loss:  0.0002528398896780952\n",
      "epoch:  4000 loss:  0.000252818978575912\n",
      "epoch:  4010 loss:  0.00025279808323830366\n",
      "epoch:  4020 loss:  0.00025277724550202646\n",
      "epoch:  4030 loss:  0.00025275637502393994\n",
      "epoch:  4040 loss:  0.0002527355281927157\n",
      "epoch:  4050 loss:  0.00025271471228431136\n",
      "epoch:  4060 loss:  0.00025269390365186456\n",
      "epoch:  4070 loss:  0.0002526730980510668\n",
      "epoch:  4080 loss:  0.00025265232640473795\n",
      "epoch:  4090 loss:  0.0002526315432381428\n",
      "epoch:  4100 loss:  0.00025261078189942054\n",
      "epoch:  4110 loss:  0.00025259004056958173\n",
      "epoch:  4120 loss:  0.00025256933076889254\n",
      "epoch:  4130 loss:  0.00025254856154788285\n",
      "epoch:  4140 loss:  0.00025252788206368376\n",
      "epoch:  4150 loss:  0.00025250720500480384\n",
      "epoch:  4160 loss:  0.000252486561294063\n",
      "epoch:  4170 loss:  0.0002524658902984811\n",
      "epoch:  4180 loss:  0.00025244529873210314\n",
      "epoch:  4190 loss:  0.0002524246362251385\n",
      "epoch:  4200 loss:  0.0002524040016093447\n",
      "epoch:  4210 loss:  0.0002523834021606793\n",
      "epoch:  4220 loss:  0.0002523627578436087\n",
      "epoch:  4230 loss:  0.00025234216506457113\n",
      "epoch:  4240 loss:  0.0002523215965387256\n",
      "epoch:  4250 loss:  0.0002523010049723477\n",
      "epoch:  4260 loss:  0.000252280424926236\n",
      "epoch:  4270 loss:  0.0002522598103193256\n",
      "epoch:  4280 loss:  0.00025223927574794897\n",
      "epoch:  4290 loss:  0.0002522187278373167\n",
      "epoch:  4300 loss:  0.0002521981835646632\n",
      "epoch:  4310 loss:  0.00025217762170844554\n",
      "epoch:  4320 loss:  0.0002521571307928146\n",
      "epoch:  4330 loss:  0.000252136593189789\n",
      "epoch:  4340 loss:  0.0002521160834779342\n",
      "epoch:  4350 loss:  0.0002520955713407602\n",
      "epoch:  4360 loss:  0.0002520750986150233\n",
      "epoch:  4370 loss:  0.0002520545701069447\n",
      "epoch:  4380 loss:  0.0002520340688837071\n",
      "epoch:  4390 loss:  0.0002520135864566934\n",
      "epoch:  4400 loss:  0.00025199310342334985\n",
      "epoch:  4410 loss:  0.00025197262705963414\n",
      "epoch:  4420 loss:  0.0002519521870757065\n",
      "epoch:  4430 loss:  0.00025193173920949147\n",
      "epoch:  4440 loss:  0.0002519113180217876\n",
      "epoch:  4450 loss:  0.0002518909222999355\n",
      "epoch:  4460 loss:  0.0002518704714020714\n",
      "epoch:  4470 loss:  0.0002518500514270272\n",
      "epoch:  4480 loss:  0.0002518296714697499\n",
      "epoch:  4490 loss:  0.0002518092799922063\n",
      "epoch:  4500 loss:  0.00025178890488556743\n",
      "epoch:  4510 loss:  0.00025176850491940667\n",
      "epoch:  4520 loss:  0.00025174812496212934\n",
      "epoch:  4530 loss:  0.00025172774015421356\n",
      "epoch:  4540 loss:  0.000251707330486776\n",
      "epoch:  4550 loss:  0.0002516869111180616\n",
      "epoch:  4560 loss:  0.00025166651418354985\n",
      "epoch:  4570 loss:  0.00025164610451611225\n",
      "epoch:  4580 loss:  0.0002516257184955369\n",
      "epoch:  4590 loss:  0.00025160532944331254\n",
      "epoch:  4600 loss:  0.0002515849652506101\n",
      "epoch:  4610 loss:  0.0002515646143971632\n",
      "epoch:  4620 loss:  0.0002515441919967998\n",
      "epoch:  4630 loss:  0.0002515238393243635\n",
      "epoch:  4640 loss:  0.0002515035090861299\n",
      "epoch:  4650 loss:  0.00025148317460358766\n",
      "epoch:  4660 loss:  0.00025146281647418317\n",
      "epoch:  4670 loss:  0.00025144249957520515\n",
      "epoch:  4680 loss:  0.00025142222390665364\n",
      "epoch:  4690 loss:  0.00025140193550517626\n",
      "epoch:  4700 loss:  0.0002513816519543373\n",
      "epoch:  4710 loss:  0.000251361360521211\n",
      "epoch:  4720 loss:  0.0002513410981919151\n",
      "epoch:  4730 loss:  0.00025132082373602316\n",
      "epoch:  4740 loss:  0.0002513006050624729\n",
      "epoch:  4750 loss:  0.0002512803621357307\n",
      "epoch:  4760 loss:  0.0002512601216343076\n",
      "epoch:  4770 loss:  0.0002512398932594806\n",
      "epoch:  4780 loss:  0.0002512197121783781\n",
      "epoch:  4790 loss:  0.0002511995177580199\n",
      "epoch:  4800 loss:  0.00025117932697564055\n",
      "epoch:  4810 loss:  0.0002511591543831552\n",
      "epoch:  4820 loss:  0.00025113895814380766\n",
      "epoch:  4830 loss:  0.00025111878373233293\n",
      "epoch:  4840 loss:  0.000251098637212029\n",
      "epoch:  4850 loss:  0.00025107845188661787\n",
      "epoch:  4860 loss:  0.0002510582914207286\n",
      "epoch:  4870 loss:  0.0002510380854801042\n",
      "epoch:  4880 loss:  0.00025101794805474736\n",
      "epoch:  4890 loss:  0.00025099774575210176\n",
      "epoch:  4900 loss:  0.00025097759074318066\n",
      "epoch:  4910 loss:  0.0002509574490735152\n",
      "epoch:  4920 loss:  0.00025093725222783786\n",
      "epoch:  4930 loss:  0.000250917142087322\n",
      "epoch:  4940 loss:  0.00025089695494292147\n",
      "epoch:  4950 loss:  0.00025087684055809706\n",
      "epoch:  4960 loss:  0.0002508566819111972\n",
      "epoch:  4970 loss:  0.00025083659299222444\n",
      "epoch:  4980 loss:  0.0002508164391959629\n",
      "epoch:  4990 loss:  0.00025079633996938355\n",
      "epoch:  5000 loss:  0.0002507762540820598\n",
      "epoch:  5010 loss:  0.000250756153036491\n",
      "epoch:  5020 loss:  0.0002507360877643805\n",
      "epoch:  5030 loss:  0.0002507160327998766\n",
      "epoch:  5040 loss:  0.00025069598874930915\n",
      "epoch:  5050 loss:  0.000250675901042996\n",
      "epoch:  5060 loss:  0.0002506558575987583\n",
      "epoch:  5070 loss:  0.00025063584083303186\n",
      "epoch:  5080 loss:  0.0002506158422571995\n",
      "epoch:  5090 loss:  0.0002505958236724837\n",
      "epoch:  5100 loss:  0.0002505758166080341\n",
      "epoch:  5110 loss:  0.00025055580166129704\n",
      "epoch:  5120 loss:  0.0002505358030854647\n",
      "epoch:  5130 loss:  0.00025051582633750513\n",
      "epoch:  5140 loss:  0.0002504958435262476\n",
      "epoch:  5150 loss:  0.00025047582615419134\n",
      "epoch:  5160 loss:  0.00025045591307086096\n",
      "epoch:  5170 loss:  0.00025043591995199677\n",
      "epoch:  5180 loss:  0.0002504159607876015\n",
      "epoch:  5190 loss:  0.000250396017387781\n",
      "epoch:  5200 loss:  0.0002503760745942903\n",
      "epoch:  5210 loss:  0.00025035614756537444\n",
      "epoch:  5220 loss:  0.00025033618840097915\n",
      "epoch:  5230 loss:  0.0002503162431821693\n",
      "epoch:  5240 loss:  0.0002502963549583607\n",
      "epoch:  5250 loss:  0.00025027642853577464\n",
      "epoch:  5260 loss:  0.0002502564954435608\n",
      "epoch:  5270 loss:  0.0002502365969121456\n",
      "epoch:  5280 loss:  0.00025021670080604963\n",
      "epoch:  5290 loss:  0.0002501968441113907\n",
      "epoch:  5300 loss:  0.0002501769728648166\n",
      "epoch:  5310 loss:  0.00025015717195249937\n",
      "epoch:  5320 loss:  0.00025013728069704183\n",
      "epoch:  5330 loss:  0.00025011744219227694\n",
      "epoch:  5340 loss:  0.00025009763764198095\n",
      "epoch:  5350 loss:  0.0002500778318790253\n",
      "epoch:  5360 loss:  0.00025005799155527103\n",
      "epoch:  5370 loss:  0.00025003825915822137\n",
      "epoch:  5380 loss:  0.0002500184885623942\n",
      "epoch:  5390 loss:  0.00024999869068172603\n",
      "epoch:  5400 loss:  0.0002499790043657413\n",
      "epoch:  5410 loss:  0.00024995928833959624\n",
      "epoch:  5420 loss:  0.00024993954381595057\n",
      "epoch:  5430 loss:  0.00024991984901134856\n",
      "epoch:  5440 loss:  0.0002499001396548313\n",
      "epoch:  5450 loss:  0.00024988045818948496\n",
      "epoch:  5460 loss:  0.0002498607682355214\n",
      "epoch:  5470 loss:  0.0002498411201183141\n",
      "epoch:  5480 loss:  0.00024982140227317967\n",
      "epoch:  5490 loss:  0.0002498017468800147\n",
      "epoch:  5500 loss:  0.00024978210664509487\n",
      "epoch:  5510 loss:  0.00024976244822028093\n",
      "epoch:  5520 loss:  0.00024974285224743653\n",
      "epoch:  5530 loss:  0.00024972321140618686\n",
      "epoch:  5540 loss:  0.0002497036057320656\n",
      "epoch:  5550 loss:  0.0002496840285554451\n",
      "epoch:  5560 loss:  0.00024966443197627086\n",
      "epoch:  5570 loss:  0.0002496448493426821\n",
      "epoch:  5580 loss:  0.00024962529763191316\n",
      "epoch:  5590 loss:  0.00024960573318821844\n",
      "epoch:  5600 loss:  0.00024958618450909853\n",
      "epoch:  5610 loss:  0.000249566637648968\n",
      "epoch:  5620 loss:  0.0002495471114040508\n",
      "epoch:  5630 loss:  0.0002495275863717931\n",
      "epoch:  5640 loss:  0.00024950806982815266\n",
      "epoch:  5650 loss:  0.00024948851084142615\n",
      "epoch:  5660 loss:  0.00024946902340161614\n",
      "epoch:  5670 loss:  0.0002494495535453704\n",
      "epoch:  5680 loss:  0.0002494300576169432\n",
      "epoch:  5690 loss:  0.00024941058351638884\n",
      "epoch:  5700 loss:  0.0002493911621665272\n",
      "epoch:  5710 loss:  0.0002493717311153887\n",
      "epoch:  5720 loss:  0.00024935227217307937\n",
      "epoch:  5730 loss:  0.0002493328453662495\n",
      "epoch:  5740 loss:  0.00024931344159995206\n",
      "epoch:  5750 loss:  0.000249294015399452\n",
      "epoch:  5760 loss:  0.00024927460375086713\n",
      "epoch:  5770 loss:  0.00024925522969472996\n",
      "epoch:  5780 loss:  0.0002492358362360392\n",
      "epoch:  5790 loss:  0.000249216443990008\n",
      "epoch:  5800 loss:  0.00024919704083004035\n",
      "epoch:  5810 loss:  0.0002491776813258184\n",
      "epoch:  5820 loss:  0.00024915833031021367\n",
      "epoch:  5830 loss:  0.0002491389568604063\n",
      "epoch:  5840 loss:  0.0002491195815916096\n",
      "epoch:  5850 loss:  0.0002491002633178141\n",
      "epoch:  5860 loss:  0.0002490809341300822\n",
      "epoch:  5870 loss:  0.0002490615982727225\n",
      "epoch:  5880 loss:  0.0002490422933381827\n",
      "epoch:  5890 loss:  0.00024902299386061105\n",
      "epoch:  5900 loss:  0.0002490037028716567\n",
      "epoch:  5910 loss:  0.0002489843864168506\n",
      "epoch:  5920 loss:  0.00024896511301146046\n",
      "epoch:  5930 loss:  0.00024894583172378287\n",
      "epoch:  5940 loss:  0.00024892655286142446\n",
      "epoch:  5950 loss:  0.00024890729097630054\n",
      "epoch:  5960 loss:  0.00024888806001399644\n",
      "epoch:  5970 loss:  0.00024886882056307513\n",
      "epoch:  5980 loss:  0.0002488496150666227\n",
      "epoch:  5990 loss:  0.0002488304295790537\n",
      "epoch:  6000 loss:  0.00024881125379276153\n",
      "epoch:  6010 loss:  0.00024879203253173426\n",
      "epoch:  6020 loss:  0.0002487728791796447\n",
      "epoch:  6030 loss:  0.00024875372885920416\n",
      "epoch:  6040 loss:  0.00024873456519950804\n",
      "epoch:  6050 loss:  0.00024871535242709797\n",
      "epoch:  6060 loss:  0.0002486962136269237\n",
      "epoch:  6070 loss:  0.0002486770693697811\n",
      "epoch:  6080 loss:  0.0002486579493658307\n",
      "epoch:  6090 loss:  0.00024863880935299676\n",
      "epoch:  6100 loss:  0.0002486197554389946\n",
      "epoch:  6110 loss:  0.0002486006281590865\n",
      "epoch:  6120 loss:  0.0002485815366526367\n",
      "epoch:  6130 loss:  0.00024856243847655907\n",
      "epoch:  6140 loss:  0.0002485433566713861\n",
      "epoch:  6150 loss:  0.0002485242875991389\n",
      "epoch:  6160 loss:  0.00024850526885226526\n",
      "epoch:  6170 loss:  0.00024848622221422073\n",
      "epoch:  6180 loss:  0.00024846717072553776\n",
      "epoch:  6190 loss:  0.0002484481944217502\n",
      "epoch:  6200 loss:  0.0002484292059913666\n",
      "epoch:  6210 loss:  0.000248410229687579\n",
      "epoch:  6220 loss:  0.00024839129764586687\n",
      "epoch:  6230 loss:  0.00024837230618383427\n",
      "epoch:  6240 loss:  0.00024835336686616455\n",
      "epoch:  6250 loss:  0.00024833445180168684\n",
      "epoch:  6260 loss:  0.00024831552521694295\n",
      "epoch:  6270 loss:  0.0002482966653284772\n",
      "epoch:  6280 loss:  0.00024827774783868034\n",
      "epoch:  6290 loss:  0.00024825884550712846\n",
      "epoch:  6300 loss:  0.0002482398867869051\n",
      "epoch:  6310 loss:  0.0002482209177590751\n",
      "epoch:  6320 loss:  0.00024820199723762926\n",
      "epoch:  6330 loss:  0.000248183077928843\n",
      "epoch:  6340 loss:  0.00024816414224915206\n",
      "epoch:  6350 loss:  0.0002481451968681843\n",
      "epoch:  6360 loss:  0.00024812626725179143\n",
      "epoch:  6370 loss:  0.00024810738977976143\n",
      "epoch:  6380 loss:  0.00024808846683299635\n",
      "epoch:  6390 loss:  0.00024806953539761406\n",
      "epoch:  6400 loss:  0.00024805066277622245\n",
      "epoch:  6410 loss:  0.0002480317634763196\n",
      "epoch:  6420 loss:  0.00024801291328913067\n",
      "epoch:  6430 loss:  0.00024799401277656824\n",
      "epoch:  6440 loss:  0.00024797511226400576\n",
      "epoch:  6450 loss:  0.0002479562129641029\n",
      "epoch:  6460 loss:  0.00024793733427941334\n",
      "epoch:  6470 loss:  0.00024791847317828797\n",
      "epoch:  6480 loss:  0.0002478996557329083\n",
      "epoch:  6490 loss:  0.0002478807800798677\n",
      "epoch:  6500 loss:  0.0002478619201914019\n",
      "epoch:  6510 loss:  0.0002478430803118196\n",
      "epoch:  6520 loss:  0.0002478242495271843\n",
      "epoch:  6530 loss:  0.00024780545512233704\n",
      "epoch:  6540 loss:  0.0002477866910339799\n",
      "epoch:  6550 loss:  0.00024776785539870616\n",
      "epoch:  6560 loss:  0.000247749097373647\n",
      "epoch:  6570 loss:  0.0002477302756839587\n",
      "epoch:  6580 loss:  0.0002477114515689512\n",
      "epoch:  6590 loss:  0.0002476926165400073\n",
      "epoch:  6600 loss:  0.00024767380697691504\n",
      "epoch:  6610 loss:  0.0002476549780112691\n",
      "epoch:  6620 loss:  0.0002476361787557835\n",
      "epoch:  6630 loss:  0.00024761736919269123\n",
      "epoch:  6640 loss:  0.00024759861177396186\n",
      "epoch:  6650 loss:  0.00024757982525140204\n",
      "epoch:  6660 loss:  0.00024756105509974685\n",
      "epoch:  6670 loss:  0.00024754225463160157\n",
      "epoch:  6680 loss:  0.00024752352995468147\n",
      "epoch:  6690 loss:  0.0002475047604093561\n",
      "epoch:  6700 loss:  0.0002474860217868506\n",
      "epoch:  6710 loss:  0.00024746724011492915\n",
      "epoch:  6720 loss:  0.0002474485336279031\n",
      "epoch:  6730 loss:  0.0002474298216839088\n",
      "epoch:  6740 loss:  0.0002474110509259238\n",
      "epoch:  6750 loss:  0.00024739240446554806\n",
      "epoch:  6760 loss:  0.0002473737088924584\n",
      "epoch:  6770 loss:  0.000247355021807986\n",
      "epoch:  6780 loss:  0.00024733639050585526\n",
      "epoch:  6790 loss:  0.0002473177652670226\n",
      "epoch:  6800 loss:  0.00024729908727749716\n",
      "epoch:  6810 loss:  0.0002472804141386102\n",
      "epoch:  6820 loss:  0.00024726178283647943\n",
      "epoch:  6830 loss:  0.0002472431193988693\n",
      "epoch:  6840 loss:  0.00024722453599679284\n",
      "epoch:  6850 loss:  0.00024720592894785415\n",
      "epoch:  6860 loss:  0.0002471873685863102\n",
      "epoch:  6870 loss:  0.0002471688009488086\n",
      "epoch:  6880 loss:  0.0002471502411935944\n",
      "epoch:  6890 loss:  0.0002471316723434332\n",
      "epoch:  6900 loss:  0.00024711310349327203\n",
      "epoch:  6910 loss:  0.0002470945364621002\n",
      "epoch:  6920 loss:  0.00024707597367523704\n",
      "epoch:  6930 loss:  0.000247057379965554\n",
      "epoch:  6940 loss:  0.0002470388062647544\n",
      "epoch:  6950 loss:  0.0002470202434778912\n",
      "epoch:  6960 loss:  0.00024700168675432604\n",
      "epoch:  6970 loss:  0.0002469831591345913\n",
      "epoch:  6980 loss:  0.00024696460241102614\n",
      "epoch:  6990 loss:  0.0002469460735786318\n",
      "epoch:  7000 loss:  0.00024692752049304545\n",
      "epoch:  7010 loss:  0.00024690899833027896\n",
      "epoch:  7020 loss:  0.0002468904725295336\n",
      "epoch:  7030 loss:  0.00024687191459330887\n",
      "epoch:  7040 loss:  0.00024685339424953173\n",
      "epoch:  7050 loss:  0.0002468348375259666\n",
      "epoch:  7060 loss:  0.00024681629838596564\n",
      "epoch:  7070 loss:  0.0002467978144219766\n",
      "epoch:  7080 loss:  0.0002467792861959121\n",
      "epoch:  7090 loss:  0.00024676078707367804\n",
      "epoch:  7100 loss:  0.0002467422685488903\n",
      "epoch:  7110 loss:  0.0002467237670013371\n",
      "epoch:  7120 loss:  0.0002467052903133056\n",
      "epoch:  7130 loss:  0.0002466867978606994\n",
      "epoch:  7140 loss:  0.0002466683351182534\n",
      "epoch:  7150 loss:  0.00024664987419479684\n",
      "epoch:  7160 loss:  0.0002466314472258091\n",
      "epoch:  7170 loss:  0.00024661295537953265\n",
      "epoch:  7180 loss:  0.00024659450476368267\n",
      "epoch:  7190 loss:  0.00024657600685410824\n",
      "epoch:  7200 loss:  0.00024655759140538674\n",
      "epoch:  7210 loss:  0.0002465391310882599\n",
      "epoch:  7220 loss:  0.0002465207083635808\n",
      "epoch:  7230 loss:  0.00024650225289709243\n",
      "epoch:  7240 loss:  0.00024648378287868883\n",
      "epoch:  7250 loss:  0.0002464653546970415\n",
      "epoch:  7260 loss:  0.0002464469216647558\n",
      "epoch:  7270 loss:  0.00024642853774518397\n",
      "epoch:  7280 loss:  0.0002464100937989618\n",
      "epoch:  7290 loss:  0.00024639171169837937\n",
      "epoch:  7300 loss:  0.000246373262295189\n",
      "epoch:  7310 loss:  0.00024635490323513903\n",
      "epoch:  7320 loss:  0.00024633656842828106\n",
      "epoch:  7330 loss:  0.00024631817602009204\n",
      "epoch:  7340 loss:  0.00024629981817270163\n",
      "epoch:  7350 loss:  0.00024628151853297214\n",
      "epoch:  7360 loss:  0.00024626313885770895\n",
      "epoch:  7370 loss:  0.0002462447973812232\n",
      "epoch:  7380 loss:  0.00024622646681867383\n",
      "epoch:  7390 loss:  0.00024620810775862384\n",
      "epoch:  7400 loss:  0.0002461897462732547\n",
      "epoch:  7410 loss:  0.00024617141389171593\n",
      "epoch:  7420 loss:  0.0002461530887861348\n",
      "epoch:  7430 loss:  0.00024613479096539476\n",
      "epoch:  7440 loss:  0.0002461165022396017\n",
      "epoch:  7450 loss:  0.0002460982359480113\n",
      "epoch:  7460 loss:  0.0002460798811322699\n",
      "epoch:  7470 loss:  0.00024606161180903047\n",
      "epoch:  7480 loss:  0.0002460433133819606\n",
      "epoch:  7490 loss:  0.00024602506224861526\n",
      "epoch:  7500 loss:  0.0002460067814051096\n",
      "epoch:  7510 loss:  0.000245988524814796\n",
      "epoch:  7520 loss:  0.0002459702748941102\n",
      "epoch:  7530 loss:  0.0002459520310367225\n",
      "epoch:  7540 loss:  0.0002459337799033771\n",
      "epoch:  7550 loss:  0.00024591556333083037\n",
      "epoch:  7560 loss:  0.0002458973722241353\n",
      "epoch:  7570 loss:  0.0002458791562579184\n",
      "epoch:  7580 loss:  0.00024586096151324455\n",
      "epoch:  7590 loss:  0.00024584274312170845\n",
      "epoch:  7600 loss:  0.0002458245726302266\n",
      "epoch:  7610 loss:  0.00024580637788555276\n",
      "epoch:  7620 loss:  0.00024578821467002854\n",
      "epoch:  7630 loss:  0.000245769976269609\n",
      "epoch:  7640 loss:  0.0002457517712173285\n",
      "epoch:  7650 loss:  0.00024573354433717515\n",
      "epoch:  7660 loss:  0.00024571537202670396\n",
      "epoch:  7670 loss:  0.0002456971906212857\n",
      "epoch:  7680 loss:  0.0002456790310437403\n",
      "epoch:  7690 loss:  0.0002456608726788545\n",
      "epoch:  7700 loss:  0.0002456427397798204\n",
      "epoch:  7710 loss:  0.00024562462082637165\n",
      "epoch:  7720 loss:  0.0002456064952032951\n",
      "epoch:  7730 loss:  0.00024558833926372853\n",
      "epoch:  7740 loss:  0.0002455702239482586\n",
      "epoch:  7750 loss:  0.00024555208316693705\n",
      "epoch:  7760 loss:  0.0002455339684577969\n",
      "epoch:  7770 loss:  0.00024551587678918924\n",
      "epoch:  7780 loss:  0.00024549776208004914\n",
      "epoch:  7790 loss:  0.0002454796704114415\n",
      "epoch:  7800 loss:  0.000245461572073206\n",
      "epoch:  7810 loss:  0.00024544351193374797\n",
      "epoch:  7820 loss:  0.0002454254123828529\n",
      "epoch:  7830 loss:  0.0002454073595193525\n",
      "epoch:  7840 loss:  0.00024538932605840574\n",
      "epoch:  7850 loss:  0.0002453713374658643\n",
      "epoch:  7860 loss:  0.0002453532906656619\n",
      "epoch:  7870 loss:  0.0002453352857022158\n",
      "epoch:  7880 loss:  0.0002453172673995141\n",
      "epoch:  7890 loss:  0.0002452992697120256\n",
      "epoch:  7900 loss:  0.0002452813047663464\n",
      "epoch:  7910 loss:  0.0002452632943459321\n",
      "epoch:  7920 loss:  0.0002452452408761019\n",
      "epoch:  7930 loss:  0.00024522712677329156\n",
      "epoch:  7940 loss:  0.0002452090884617064\n",
      "epoch:  7950 loss:  0.000245191011345014\n",
      "epoch:  7960 loss:  0.00024517293907895993\n",
      "epoch:  7970 loss:  0.0002451548680255655\n",
      "epoch:  7980 loss:  0.0002451368685190876\n",
      "epoch:  7990 loss:  0.0002451187974656932\n",
      "epoch:  8000 loss:  0.00024510074824017164\n",
      "epoch:  8010 loss:  0.00024508265899688314\n",
      "epoch:  8020 loss:  0.000245064629780245\n",
      "epoch:  8030 loss:  0.0002450465738850956\n",
      "epoch:  8040 loss:  0.00024502857801659655\n",
      "epoch:  8050 loss:  0.00024501056820251205\n",
      "epoch:  8060 loss:  0.00024499262265938643\n",
      "epoch:  8070 loss:  0.00024497461769594037\n",
      "epoch:  8080 loss:  0.0002449566394110055\n",
      "epoch:  8090 loss:  0.00024493861565133557\n",
      "epoch:  8100 loss:  0.0002449208222969901\n",
      "epoch:  8110 loss:  0.0002449030343996128\n",
      "epoch:  8120 loss:  0.0002448853041035666\n",
      "epoch:  8130 loss:  0.00024486750711124233\n",
      "epoch:  8140 loss:  0.0002448497780278558\n",
      "epoch:  8150 loss:  0.00024483205864574603\n",
      "epoch:  8160 loss:  0.00024481430773448665\n",
      "epoch:  8170 loss:  0.0002447965853207279\n",
      "epoch:  8180 loss:  0.0002447788974677678\n",
      "epoch:  8190 loss:  0.00024476122052874416\n",
      "epoch:  8200 loss:  0.0002447434920516874\n",
      "epoch:  8210 loss:  0.0002447257902531419\n",
      "epoch:  8220 loss:  0.00024470812422805466\n",
      "epoch:  8230 loss:  0.00024469042909913696\n",
      "epoch:  8240 loss:  0.00024467277338165633\n",
      "epoch:  8250 loss:  0.00024465507643374923\n",
      "epoch:  8260 loss:  0.0002446374031327044\n",
      "epoch:  8270 loss:  0.0002446197546911814\n",
      "epoch:  8280 loss:  0.000244602110493967\n",
      "epoch:  8290 loss:  0.00024458442627898574\n",
      "epoch:  8300 loss:  0.0002445668154299104\n",
      "epoch:  8310 loss:  0.0002445491627440788\n",
      "epoch:  8320 loss:  0.0002445315209721836\n",
      "epoch:  8330 loss:  0.00024451388405092683\n",
      "epoch:  8340 loss:  0.00024449626107525546\n",
      "epoch:  8350 loss:  0.00024447861687804107\n",
      "epoch:  8360 loss:  0.00024446095812891144\n",
      "epoch:  8370 loss:  0.00024444335940643214\n",
      "epoch:  8380 loss:  0.00024442571945352637\n",
      "epoch:  8390 loss:  0.00024440811891205766\n",
      "epoch:  8400 loss:  0.0002443905007870247\n",
      "epoch:  8410 loss:  0.00024437287417337455\n",
      "epoch:  8420 loss:  0.00024435525422935217\n",
      "epoch:  8430 loss:  0.0002443376615701709\n",
      "epoch:  8440 loss:  0.0002443200228299247\n",
      "epoch:  8450 loss:  0.0002443024398720202\n",
      "epoch:  8460 loss:  0.0002442848096203913\n",
      "epoch:  8470 loss:  0.00024426724182073184\n",
      "epoch:  8480 loss:  0.00024424966068181675\n",
      "epoch:  8490 loss:  0.0002442320292175282\n",
      "epoch:  8500 loss:  0.0002442144444406343\n",
      "epoch:  8510 loss:  0.00024419667655214045\n",
      "epoch:  8520 loss:  0.0002441789492877433\n",
      "epoch:  8530 loss:  0.00024416121474738855\n",
      "epoch:  8540 loss:  0.0002441435675185251\n",
      "epoch:  8550 loss:  0.0002441259002807783\n",
      "epoch:  8560 loss:  0.00024410822516074404\n",
      "epoch:  8570 loss:  0.00024409055246602898\n",
      "epoch:  8580 loss:  0.00024407286340040932\n",
      "epoch:  8590 loss:  0.00024405518949303465\n",
      "epoch:  8600 loss:  0.0002440375719743315\n",
      "epoch:  8610 loss:  0.0002440199132252019\n",
      "epoch:  8620 loss:  0.0002440022781229345\n",
      "epoch:  8630 loss:  0.00024398465029662475\n",
      "epoch:  8640 loss:  0.00024396706612606067\n",
      "epoch:  8650 loss:  0.00024394942435416547\n",
      "epoch:  8660 loss:  0.00024393180986711135\n",
      "epoch:  8670 loss:  0.00024391422023957907\n",
      "epoch:  8680 loss:  0.00024389663303736597\n",
      "epoch:  8690 loss:  0.0002438789979350986\n",
      "epoch:  8700 loss:  0.00024386144044304578\n",
      "epoch:  8710 loss:  0.00024384382716865124\n",
      "epoch:  8720 loss:  0.0002438262636133004\n",
      "epoch:  8730 loss:  0.00024380865700853369\n",
      "epoch:  8740 loss:  0.0002437910722316398\n",
      "epoch:  8750 loss:  0.00024377348563575651\n",
      "epoch:  8760 loss:  0.00024375591783609707\n",
      "epoch:  8770 loss:  0.0002437383451857992\n",
      "epoch:  8780 loss:  0.00024372078102411857\n",
      "epoch:  8790 loss:  0.00024370319079025649\n",
      "epoch:  8800 loss:  0.00024368567392230034\n",
      "epoch:  8810 loss:  0.00024366813219482233\n",
      "epoch:  8820 loss:  0.0002436506207838344\n",
      "epoch:  8830 loss:  0.00024363309300194183\n",
      "epoch:  8840 loss:  0.00024361559129223073\n",
      "epoch:  8850 loss:  0.00024359807078629578\n",
      "epoch:  8860 loss:  0.00024358053027147739\n",
      "epoch:  8870 loss:  0.00024356302795543647\n",
      "epoch:  8880 loss:  0.00024354551472545913\n",
      "epoch:  8890 loss:  0.0002435280142284076\n",
      "epoch:  8900 loss:  0.00024351044521608856\n",
      "epoch:  8910 loss:  0.00024349298231148472\n",
      "epoch:  8920 loss:  0.00024347547938911399\n",
      "epoch:  8930 loss:  0.00024345794675658303\n",
      "epoch:  8940 loss:  0.000243440401997456\n",
      "epoch:  8950 loss:  0.00024342287785354225\n",
      "epoch:  8960 loss:  0.00024340529732095698\n",
      "epoch:  8970 loss:  0.00024338767252629623\n",
      "epoch:  8980 loss:  0.0002433701568709997\n",
      "epoch:  8990 loss:  0.00024335260604857467\n",
      "epoch:  9000 loss:  0.0002433350722033841\n",
      "epoch:  9010 loss:  0.00024331752138095908\n",
      "epoch:  9020 loss:  0.00024330003179784399\n",
      "epoch:  9030 loss:  0.00024328256161728254\n",
      "epoch:  9040 loss:  0.0002432651065949661\n",
      "epoch:  9050 loss:  0.00024324767643217152\n",
      "epoch:  9060 loss:  0.000243230273554218\n",
      "epoch:  9070 loss:  0.00024321280883062477\n",
      "epoch:  9080 loss:  0.0002431954453641083\n",
      "epoch:  9090 loss:  0.0002431779957987601\n",
      "epoch:  9100 loss:  0.00024316058867649795\n",
      "epoch:  9110 loss:  0.00024314318640487423\n",
      "epoch:  9120 loss:  0.0002431257823142611\n",
      "epoch:  9130 loss:  0.00024310834426917913\n",
      "epoch:  9140 loss:  0.00024309095715580042\n",
      "epoch:  9150 loss:  0.0002430735530651873\n",
      "epoch:  9160 loss:  0.00024305612835936094\n",
      "epoch:  9170 loss:  0.00024303875034092925\n",
      "epoch:  9180 loss:  0.00024302141779723266\n",
      "epoch:  9190 loss:  0.00024300407919023806\n",
      "epoch:  9200 loss:  0.00024298673391361567\n",
      "epoch:  9210 loss:  0.00024296938075470584\n",
      "epoch:  9220 loss:  0.00024295211672627678\n",
      "epoch:  9230 loss:  0.00024293479449018682\n",
      "epoch:  9240 loss:  0.00024291746194649022\n",
      "epoch:  9250 loss:  0.0002429001148508784\n",
      "epoch:  9260 loss:  0.0002428828192932997\n",
      "epoch:  9270 loss:  0.00024286553404332759\n",
      "epoch:  9280 loss:  0.0002428482948744204\n",
      "epoch:  9290 loss:  0.00024283105146120457\n",
      "epoch:  9300 loss:  0.0002428137874327755\n",
      "epoch:  9310 loss:  0.00024279652825498488\n",
      "epoch:  9320 loss:  0.00024277925998224723\n",
      "epoch:  9330 loss:  0.00024276203960956386\n",
      "epoch:  9340 loss:  0.00024274482590650828\n",
      "epoch:  9350 loss:  0.00024272758067430308\n",
      "epoch:  9360 loss:  0.00024271037606619453\n",
      "epoch:  9370 loss:  0.00024269318904165024\n",
      "epoch:  9380 loss:  0.00024267594926641323\n",
      "epoch:  9390 loss:  0.00024265871131016561\n",
      "epoch:  9400 loss:  0.00024264150488306768\n",
      "epoch:  9410 loss:  0.0002426243360484174\n",
      "epoch:  9420 loss:  0.00024260716296945853\n",
      "epoch:  9430 loss:  0.00024258993228916856\n",
      "epoch:  9440 loss:  0.00024257278103808252\n",
      "epoch:  9450 loss:  0.00024255557036667597\n",
      "epoch:  9460 loss:  0.0002425384009256959\n",
      "epoch:  9470 loss:  0.00024252124118599264\n",
      "epoch:  9480 loss:  0.0002425040329399053\n",
      "epoch:  9490 loss:  0.00024248688593312787\n",
      "epoch:  9500 loss:  0.00024246970375922197\n",
      "epoch:  9510 loss:  0.00024245254098786972\n",
      "epoch:  9520 loss:  0.00024243540550135853\n",
      "epoch:  9530 loss:  0.00024241832882883804\n",
      "epoch:  9540 loss:  0.0002424012309347745\n",
      "epoch:  9550 loss:  0.00024238406270645405\n",
      "epoch:  9560 loss:  0.0002423669575364329\n",
      "epoch:  9570 loss:  0.00024234984933476275\n",
      "epoch:  9580 loss:  0.0002423327987344237\n",
      "epoch:  9590 loss:  0.00024231574025179725\n",
      "epoch:  9600 loss:  0.0002422987399768317\n",
      "epoch:  9610 loss:  0.00024228162995617217\n",
      "epoch:  9620 loss:  0.00024226454418870466\n",
      "epoch:  9630 loss:  0.00024224758089985698\n",
      "epoch:  9640 loss:  0.0002422304605715908\n",
      "epoch:  9650 loss:  0.00024221343483077362\n",
      "epoch:  9660 loss:  0.00024219642727985047\n",
      "epoch:  9670 loss:  0.00024217937789217103\n",
      "epoch:  9680 loss:  0.00024216234366273662\n",
      "epoch:  9690 loss:  0.0002421453051889936\n",
      "epoch:  9700 loss:  0.00024212830188237908\n",
      "epoch:  9710 loss:  0.00024211126158964666\n",
      "epoch:  9720 loss:  0.00024209421826526523\n",
      "epoch:  9730 loss:  0.0002420772258725871\n",
      "epoch:  9740 loss:  0.00024206020073809972\n",
      "epoch:  9750 loss:  0.00024204324411887987\n",
      "epoch:  9760 loss:  0.00024202621292109447\n",
      "epoch:  9770 loss:  0.0002420091908182561\n",
      "epoch:  9780 loss:  0.00024199220206355676\n",
      "epoch:  9790 loss:  0.00024197516965311175\n",
      "epoch:  9800 loss:  0.0002419582051516045\n",
      "epoch:  9810 loss:  0.0002419411757728085\n",
      "epoch:  9820 loss:  0.000241924208845982\n",
      "epoch:  9830 loss:  0.0002419072219102721\n",
      "epoch:  9840 loss:  0.0002418902428568496\n",
      "epoch:  9850 loss:  0.00024187328866294897\n",
      "epoch:  9860 loss:  0.00024185631385383508\n",
      "epoch:  9870 loss:  0.00024183934328902978\n",
      "epoch:  9880 loss:  0.00024182240364704435\n",
      "epoch:  9890 loss:  0.00024180540276574902\n",
      "epoch:  9900 loss:  0.00024178844493386956\n",
      "epoch:  9910 loss:  0.0002417714252563504\n",
      "epoch:  9920 loss:  0.0002417544413522895\n",
      "epoch:  9930 loss:  0.00024173745866088817\n",
      "epoch:  9940 loss:  0.00024172046566188024\n",
      "epoch:  9950 loss:  0.00024170349267175575\n",
      "epoch:  9960 loss:  0.0002416864851208326\n",
      "epoch:  9970 loss:  0.00024166951516235713\n",
      "epoch:  9980 loss:  0.0002416525221633492\n",
      "epoch:  9990 loss:  0.0002416354867212552\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import svm\n",
    "import torch.optim as optim\n",
    "\n",
    "# df = pd.read_csv(\"D:/User/Bot_C/Res/scripts/data_control.csv\")\n",
    "\n",
    "# print(y)\n",
    "PATH = \"D:/User/DLBot/scripts/model/model_arm.pt\"\n",
    "learning_rate = 0.001\n",
    "model = ANN()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"{total_params:,} total parameters.\")\n",
    "# total_trainable_params = sum(\n",
    "#     p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"{total_trainable_params:,} training parameters.\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# Loss function.\n",
    "# losses = nn.CrossEntropyLoss()\n",
    "losses = nn.MSELoss()\n",
    "loss = []\n",
    "# evaluate a model using repeated k-fold cross-validation\n",
    "for i in range(10000):\n",
    "    train_loss = study(model, X_train,y_train, optimizer, losses, device)\n",
    "    loss.append(train_loss)\n",
    "    if i%10==0:\n",
    "        print('epoch: ',i, 'loss: ', train_loss)\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# path = 'D:/User/DLBot/scripts/data/log.csv'\n",
    "# df = pd.read_csv(path)\n",
    "\n",
    "# df2 = pd.DataFrame(loss, columns=['loss'])\n",
    "# df2 = pd.concat([df, df2])\n",
    "# df2.to_csv(\n",
    "#     path, index=False)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"D:/User/Bot_C/Res/scripts/data_control.csv\")\n",
    "data = np.array(df_test)\n",
    "X_t = np.array(data[:,0:8], dtype=np.double) \n",
    "y_t = np.array(data[:,8:12], dtype=np.double)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cacular import *\n",
    "data_create = []\n",
    "for idx, dta in enumerate(X_t):\n",
    "    box_1, box_2 = dta[:4], dta[4:]\n",
    "    data_frame = np.hstack(\n",
    "                        [center_box(box_1), center_box(box_2), y_t[idx]])\n",
    "    data_create.append(data_frame)\n",
    "df = pd.read_csv(\"D:/User/Bot_C/Res/scripts/control.csv\")\n",
    "df2 = pd.DataFrame(data_create, columns=['c11', 'c12', 'c21', 'c22', 'q1', 'q2', 'q3', 'q4'])\n",
    "df2 = pd.concat([df, df2])\n",
    "df2.to_csv(\n",
    "    \"D:/User/Bot_C/Res/scripts/control.csv\", index=False)\n",
    "X_t\n",
    "data_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model = ANN()\n",
    "the_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104,  44,  63,  11],\n",
       "       [ 95,  50,  69,  12],\n",
       "       [ 92,  49,  72,  12],\n",
       "       [ 85,  45,  70,  11],\n",
       "       [103,  46,  56,   9],\n",
       "       [ 87,  34,  90,  12],\n",
       "       [102,  46,  64,  11],\n",
       "       [ 83,  47,  95,  10],\n",
       "       [101,  39,  80,  11],\n",
       "       [102,  46,  58,  11]], dtype=int8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = the_model(torch.tensor([X[:10]],dtype=torch.float32))*180\n",
    "ou = np.array(output.detach().numpy()[0],dtype=np.int8)\n",
    "# len(str(ou[0]))\n",
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tracker.track import *\n",
    "import cv2\n",
    "# url1 = 'http://192.168.2.110/cam-lo.jpg'\n",
    "frame = cv2.imread(\"D:/User/firmware/Screen/hinh-anh-mat-cuoi2-1.png\")\n",
    "\n",
    "speed = 0\n",
    "def callback(data):\n",
    "    global speed\n",
    "    speed = data\n",
    "cv2.namedWindow('test')\n",
    "cv2.createTrackbar('thrs1', 'test', 0, 250, callback)\n",
    "\n",
    "\n",
    "while True:\n",
    "# try:\n",
    "# cap = cv2.VideoCapture(url1)\n",
    "    print(speed)\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    cv2.waitKey(1)\n",
    "# Do whatever you want with contours\n",
    "# cv2.imshow('test', frame)\n",
    "# _, frame = cap.read()\n",
    "# box, frame, area = tracking_sort(frame)\n",
    "# idx = len(box[0])\n",
    "# print(box[0][len(box[0])-1])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_map(angle, inmax, inmin, outmax, outmin):\n",
    "    return (angle-inmin)*(outmax-outmin)/(inmax-inmin) + outmin\n",
    "\n",
    "areas = 15000\n",
    "different = 15000 - areas\n",
    "speed = convert_map(different, 15000, 0, 255, 150)\n",
    "print(speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path_csv = \"D:/User/Bot_C/Res/scripts/data/locals.csv\"\n",
    "\n",
    "idx_name = 2\n",
    "# df = pd.read_csv(path_csv)\n",
    "# data_create = [['imagel_1.png', 'imager_1.png', 'f']]\n",
    "# df2 = pd.DataFrame(data_create, columns=['imagel', 'imager', 'action'])\n",
    "name_l = \"imagel_{}.png\".format(idx_name)\n",
    "name_r = \"imager_{}.png\".format(idx_name)\n",
    "data_frame = []\n",
    "data_out = 1\n",
    "#  = control_keyboard(sv, key)\n",
    "data_frame.append(name_l)\n",
    "data_frame.append(name_r)\n",
    "data_frame.append(data_out)\n",
    "df2 = pd.DataFrame([data_frame], columns=['imagel', 'imager', 'action'])\n",
    "\n",
    "# df2 = pd.DataFrame(data_create, columns=['c11', 'c12', 'c21', 'c22', 'q1', 'q2', 'q3', 'q4'])\n",
    "# df2 = pd.concat([df, df2])\n",
    "df2.to_csv(\n",
    "    path_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_save_l = \"D:/User/data_map/left/\"\n",
    "path_save_r = \"D:/User/data_map/right/\"\n",
    "idx_name = len(os.listdir(path_save_l))\n",
    "\n",
    "print(idx_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# from model_CNN import Net\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from sklearn import preprocessing\n",
    "\n",
    "PATH = 'D:/User/DLBot/scripts/model/move_model.pt'\n",
    "# net = Net()\n",
    "# net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)\n",
    "\n",
    "\n",
    "path_csv = \"D:/User/DLBot/scripts/data/locals.csv\"\n",
    "path_save_l = \"D:/User/data_map/left/\"\n",
    "path_save_r = \"D:/User/data_map/right/\"\n",
    "\n",
    "df = pd.read_csv(path_csv)\n",
    "Train_y = df['action']\n",
    "\n",
    "# X = np.array(df['deceduti']).reshape(-1,1)\n",
    "chuyenDoi = preprocessing.LabelEncoder()\n",
    "chuyenDoi.fit(Train_y)\n",
    "Train_y = chuyenDoi.transform(Train_y)\n",
    "\n",
    "\n",
    "# df = np.array(df)\n",
    "\n",
    "# Train_X = []\n",
    "# print(Train_y)\n",
    "# for data in df:\n",
    "#     row_data = []\n",
    "#     # print(data)\n",
    "#     image1 = cv2.imread(path_save_l+data[0])\n",
    "#     image2 = cv2.imread(path_save_r+data[1])\n",
    "    \n",
    "#     row_data.append(image1)\n",
    "#     row_data.append(image2)\n",
    "    \n",
    "#     Train_X.append(row_data)\n",
    "    \n",
    "# print(np.array(Train_X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Train_X, Val_X, Train_y, Val_y = train_test_split(Train_X, Train_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Train_X), len(Val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_data(Train_X, Train_y, batch):\n",
    "    X, y = [], []\n",
    "    XX, yy = [], []\n",
    "    j=0\n",
    "    while j < len(Train_y):\n",
    "        X.append(Train_X[j])\n",
    "        y.append(Train_y[j])\n",
    "        if (j+1)%batch==0:\n",
    "            XX.append(X)\n",
    "            yy.append(y)\n",
    "            X, y = [], []\n",
    "        j+=1\n",
    "    return XX, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = create_batch_data(Train_X, Train_y, 32)\n",
    "tx = np.array(tx,dtype=np.uint8)/255\n",
    "ty = np.array(ty,dtype=np.uint8)\n",
    "tx[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(Train_y)\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()\n",
    "plt.hist(Val_y)\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, cv2\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_CNN import Net\n",
    "PATH_best = 'D:/User/DLBot/scripts/model/move_model_new_1.pt'\n",
    "\n",
    "# model = Net()\n",
    "# # model.to(device)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# error = nn.CrossEntropyLoss()\n",
    "\n",
    "# learning_rate = 0.001\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# num_epochs = 5\n",
    "# count = 0\n",
    "# # Lists for visualization of loss and accuracy \n",
    "# loss_list = []\n",
    "# iteration_list = []\n",
    "# accuracy_list = []\n",
    "\n",
    "# # Lists for knowing classwise accuracy\n",
    "# predictions_list = []\n",
    "# labels_list = []\n",
    "# predictions_list_train = []\n",
    "# max = 0\n",
    "# for epoch in range(num_epochs):\n",
    "#     correct_train = 0\n",
    "#     total_train = 0\n",
    "#     for i, data in enumerate(Train_X):\n",
    "#         # Transfering images and labels to GPU if available\n",
    "#         # images, labels = images.to(device), labels.to(device)\n",
    "#         image1 = cv2.resize(data[0], [255,255]).transpose([2,0,1])/255\n",
    "#         image2 = cv2.resize(data[1], [255,255]).transpose([2,0,1])/255\n",
    "#         inputs1, inputs2, labels = torch.tensor([image1],dtype=torch.float32), torch.tensor([image2],dtype=torch.float32), torch.tensor([Train_y[i]],dtype=torch.long)\n",
    "        \n",
    "#         # Forward pass \n",
    "#         outputs = model(inputs1, inputs2)\n",
    "#         loss = error(outputs, labels)\n",
    "#         predictions_train = torch.max(outputs, 1)[1]\n",
    "#         predictions_list_train.append(predictions_train)\n",
    "#         correct_train += (predictions_train == labels).sum()\n",
    "\n",
    "#         total_train += len(labels)\n",
    "\n",
    "#         accuracy_train = correct_train * 100 / total_train\n",
    "#         # Initializing a gradient as 0 so there is no mixing of gradient among the batches\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         #Propagating the error backward\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Optimizing the parameters\n",
    "#         optimizer.step()\n",
    "    \n",
    "#         count += 1\n",
    "#         # if not (count % 10):\n",
    "#             # print(\"Iteration: {}, Loss: {}, Accuracy: {} %, Correct: {}\".format(count, loss.data, accuracy_train, correct_train))\n",
    "#     # Testing the model\n",
    "    \n",
    "#         if not (count % 10):    # It's same as \"if count % 50 == 0\"\n",
    "#             total = 0\n",
    "#             correct = 0\n",
    "        \n",
    "#             for j, valda in enumerate(Val_X):\n",
    "#                 # images, labels = images.to(device), labels.to(device)\n",
    "#                 # labels_list.append(labels)\n",
    "            \n",
    "#                 # test = Variable(images.view(100, 1, 28, 28))\n",
    "#                 imagev1 = cv2.resize(valda[0], [255,255]).transpose([2,0,1])/255\n",
    "#                 imagev2 = cv2.resize(valda[1], [255,255]).transpose([2,0,1])/255\n",
    "#                 inputv1, inputv2, labels = torch.tensor([imagev1],dtype=torch.float32), torch.tensor([imagev2],dtype=torch.float32), torch.tensor([Val_y[j]],dtype=torch.long)\n",
    "\n",
    "#                 outputs = model(inputv1, inputv2)\n",
    "            \n",
    "#                 predictions = torch.max(outputs, 1)[1]\n",
    "#                 predictions_list.append(predictions)\n",
    "#                 correct += (predictions == labels).sum()\n",
    "            \n",
    "#                 total += len(labels)\n",
    "            \n",
    "#             accuracy = correct * 100 / total\n",
    "#             loss_list.append(loss.data)\n",
    "#             iteration_list.append(count)\n",
    "#             accuracy_list.append(accuracy)\n",
    "#             if accuracy > max:\n",
    "#                 max = accuracy \n",
    "#                 torch.save(model.state_dict(), PATH_best)\n",
    "#         if not (count % 10):\n",
    "#             print(\"Iteration: {}, Loss: {}, Accuracy: {} %, Train: {}\".format(count, loss.data, accuracy, accuracy_train))\n",
    "# torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list)\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()\n",
    "plt.plot(accuracy_list)\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(Train_X):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        image1 = cv2.resize(data[0], [255,255]).transpose([2,0,1])/255\n",
    "        image2 = cv2.resize(data[1], [255,255]).transpose([2,0,1])/255\n",
    "        inputs1, inputs2, labels = torch.tensor([image1],dtype=torch.float32), torch.tensor([image2],dtype=torch.float32), torch.tensor([Train_y[i]],dtype=torch.long)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs1, inputs2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # if i % 100 == 0:    # print every 2000 mini-batches\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\miniconda\\envs\\my_islle\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0834,  1.2180, -0.6701,  0.7060]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1.2180], grad_fn=<MaxBackward0>)\n",
      "tensor([[-0.2521,  1.3428, -0.8615,  0.4395]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1.3428], grad_fn=<MaxBackward0>)\n",
      "tensor([[-0.1449,  1.2743, -0.7487,  0.7136]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1.2743], grad_fn=<MaxBackward0>)\n",
      "tensor([[-0.4777,  0.8360, -0.3589,  0.8517]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0.8517], grad_fn=<MaxBackward0>)\n",
      "tensor([[-0.4028,  1.5360, -0.5148,  0.8071]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1.5360], grad_fn=<MaxBackward0>)\n",
      "tensor([[ 0.0516,  1.3014, -1.0872,  0.5012]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1.3014], grad_fn=<MaxBackward0>)\n",
      "tensor([[ 0.1036,  1.2389, -0.7664,  0.6307]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1.2389], grad_fn=<MaxBackward0>)\n",
      "tensor([[-0.0761,  1.1620, -0.8593,  0.6022]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1.1620], grad_fn=<MaxBackward0>)\n",
      "tensor([[-0.1467,  1.5145, -0.7758,  0.8918]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1.5145], grad_fn=<MaxBackward0>)\n",
      "tensor([[-0.3216,  1.0972, -0.6681,  1.0108]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1.0972], grad_fn=<MaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['g', 'g', 'g', 't', 'g', 'g', 'g', 'g', 'g', 'g'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH_best, map_location='cpu'))\n",
    "# Train_X[0][1]\n",
    "out = []\n",
    "for i in range (10):\n",
    "    image1 = cv2.imread(\"D:/User/data_map_0/right/imager_3.png\")\n",
    "    image2 = cv2.imread(\"D:/User/data_map_0/left/imagel_3.png\")\n",
    "    image1 = cv2.resize(image1, [256,256]).transpose([2,0,1])/256\n",
    "    image2 = cv2.resize(image2, [256,256]).transpose([2,0,1])/256\n",
    "    image1, image2 = torch.tensor([image1],dtype=torch.float32), torch.tensor([image2],dtype=torch.float32)\n",
    "    outputs = net(image1, image2)\n",
    "    print(outputs)\n",
    "    cost, predicted = torch.max(outputs, 1)\n",
    "    print(cost)\n",
    "    out.append(np.array(predicted, dtype=np.uint8)[0])\n",
    "chuyenDoi.inverse_transform(np.array(out, dtype=np.uint8))\n",
    "\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyboard\n",
    "import time\n",
    "\n",
    "shortcuts = {\n",
    "        \"Key1\":\"a\",\n",
    "        \"Key2\":\"b\",\n",
    "        \"Key3\":\"c\",\n",
    "        }\n",
    "\n",
    "def showText(text):\n",
    "    print(text)\n",
    "    \n",
    "for text, hotkey in shortcuts.items():\n",
    "    keyboard.on_press_key(hotkey, lambda _:showText(text))\n",
    "\n",
    "keyboard.on_press_key(\"d\", lambda _:showText(\"Key4\"))\n",
    "keyboard.on_press_key(\"e\", lambda _:showText(\"Key5\"))\n",
    "\n",
    "while 1:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "from model_CNNM import Net\n",
    "# import \n",
    "\n",
    "net = Net()\n",
    "\n",
    "model_graph = draw_graph(net, input_size=((1,3,255,255),(1,3,255,255)), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_ANN import ANN\n",
    "from model_CNNM import Net\n",
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "\n",
    "\n",
    "model = ANN(8)\n",
    "# model = Net()\n",
    "\n",
    "model_graph = draw_graph(model, input_size=((1,8)), expand_nested=True)\n",
    "\n",
    "# model_graph = draw_graph(model, input_size=((1,3,255,255),(1,3,255,255)), expand_nested=True)\n",
    "model_graph.visual_graph\n",
    "# graphviz.set_jupyter_format('png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_islle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
